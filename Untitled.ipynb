{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ea9ed-3814-4959-81e5-5c33ab95d4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65293ec3-f945-41ba-9c6f-47efd2e122c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = []\n",
    "value_list = []\n",
    "for i in range(1, 13):\n",
    "    key_list.append(f\"dataset_{i}\")\n",
    "    value_list.append(f\"https://www.justice.gov/epstein/doj-disclosures/data-set-{i}-files\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aea048af-f027-4d26-9f8e-b4fee6a59d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dict = {}\n",
    "for key_str, value_str in zip(key_list, value_list):\n",
    "    url_dict[key_str] = value_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce17ad7f-7f36-48fa-9eea-cf63a1d70743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_1': 'https://www.justice.gov/epstein/doj-disclosures/data-set-1-files',\n",
       " 'dataset_2': 'https://www.justice.gov/epstein/doj-disclosures/data-set-2-files',\n",
       " 'dataset_3': 'https://www.justice.gov/epstein/doj-disclosures/data-set-3-files',\n",
       " 'dataset_4': 'https://www.justice.gov/epstein/doj-disclosures/data-set-4-files',\n",
       " 'dataset_5': 'https://www.justice.gov/epstein/doj-disclosures/data-set-5-files',\n",
       " 'dataset_6': 'https://www.justice.gov/epstein/doj-disclosures/data-set-6-files',\n",
       " 'dataset_7': 'https://www.justice.gov/epstein/doj-disclosures/data-set-7-files',\n",
       " 'dataset_8': 'https://www.justice.gov/epstein/doj-disclosures/data-set-8-files',\n",
       " 'dataset_9': 'https://www.justice.gov/epstein/doj-disclosures/data-set-9-files',\n",
       " 'dataset_10': 'https://www.justice.gov/epstein/doj-disclosures/data-set-10-files',\n",
       " 'dataset_11': 'https://www.justice.gov/epstein/doj-disclosures/data-set-11-files',\n",
       " 'dataset_12': 'https://www.justice.gov/epstein/doj-disclosures/data-set-12-files'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a9697f2-28b5-456c-9d1b-f2e6cd2a57e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ dataset_1\n",
      "Found 50 PDFs\n",
      "\n",
      "üìÇ dataset_2\n",
      "Found 49 PDFs\n",
      "\n",
      "üìÇ dataset_3\n",
      "Found 49 PDFs\n",
      "\n",
      "üìÇ dataset_4\n",
      "Found 49 PDFs\n",
      "\n",
      "üìÇ dataset_5\n",
      "Found 49 PDFs\n",
      "\n",
      "üìÇ dataset_6\n",
      "Found 13 PDFs\n",
      "\n",
      "üìÇ dataset_7\n",
      "Found 17 PDFs\n",
      "\n",
      "üìÇ dataset_8\n",
      "Found 49 PDFs\n",
      "\n",
      "üìÇ dataset_9\n",
      "Found 50 PDFs\n",
      "\n",
      "üìÇ dataset_10\n",
      "Found 50 PDFs\n",
      "\n",
      "üìÇ dataset_11\n",
      "Found 50 PDFs\n",
      "\n",
      "üìÇ dataset_12\n",
      "Found 50 PDFs\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE = \"https://www.justice.gov\"\n",
    "OUT = Path(\"data/raw_pdfs\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "})\n",
    "\n",
    "for dataset_name, dataset_url in url_dict.items():\n",
    "    print(f\"\\nüìÇ {dataset_name}\")\n",
    "    dataset_dir = OUT / dataset_name\n",
    "    dataset_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    html = session.get(dataset_url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # THIS matches your Inspect screenshot\n",
    "    pdf_links = soup.select(\n",
    "        \"div.views-field-title span.field-content a[href$='.pdf']\"\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(pdf_links)} PDFs\")\n",
    "\n",
    "    for a in pdf_links:\n",
    "        pdf_url = urljoin(BASE, a[\"href\"])\n",
    "        filename = a.text.strip()\n",
    "        filepath = dataset_dir / filename\n",
    "\n",
    "        if filepath.exists():\n",
    "            continue\n",
    "\n",
    "        \n",
    "        r = session.get(pdf_url)\n",
    "        filepath.write_bytes(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cec3900-a436-48fd-b612-d3e100669906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ dataset_1\n",
      "Found 50 PDFs\n",
      "\n",
      "üìÇ dataset_2\n",
      "Found 49 PDFs\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "\n",
    "BASE = \"https://www.justice.gov\"\n",
    "OUT = Path(\"data/raw_pdfs\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "})\n",
    "\n",
    "for dataset_name, dataset_url in DATASET_URLS.items():\n",
    "    print(f\"\\nüìÇ {dataset_name}\")\n",
    "    dataset_dir = OUT / dataset_name\n",
    "    dataset_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    html = session.get(dataset_url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # THIS matches your Inspect screenshot\n",
    "    pdf_links = soup.select(\n",
    "        \"div.views-field-title span.field-content a[href$='.pdf']\"\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(pdf_links)} PDFs\")\n",
    "\n",
    "    for a in pdf_links:\n",
    "        pdf_url = urljoin(BASE, a[\"href\"])\n",
    "        filename = a.text.strip()\n",
    "        filepath = dataset_dir / filename\n",
    "\n",
    "        if filepath.exists():\n",
    "            continue\n",
    "\n",
    "        print(\"‚¨áÔ∏è\", filename)\n",
    "        r = session.get(pdf_url)\n",
    "        filepath.write_bytes(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb28a3a2-630e-4ff7-863f-576ee93fbebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification complete: 525\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "PDF_ROOT = Path(\"data/raw_pdfs\")\n",
    "OUT = Path(\"data/metadata\")\n",
    "OUT.mkdir(exist_ok=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "for pdf_path in PDF_ROOT.rglob(\"*.pdf\"):\n",
    "    has_text = False\n",
    "    has_images = False\n",
    "\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                if page.extract_text():\n",
    "                    has_text = True\n",
    "                if page.images:\n",
    "                    has_images = True\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            \"file\": str(pdf_path),\n",
    "            \"category\": \"error\",\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    if has_text and not has_images:\n",
    "        category = \"text_only\"\n",
    "    elif has_text and has_images:\n",
    "        category = \"text_plus_images\"\n",
    "    elif not has_text and has_images:\n",
    "        category = \"image_only\"\n",
    "    else:\n",
    "        category = \"empty\"\n",
    "\n",
    "    results.append({\n",
    "        \"file\": str(pdf_path),\n",
    "        \"category\": category\n",
    "    })\n",
    "\n",
    "# Save classification\n",
    "with open(OUT / \"classification.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Classification complete:\", len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09f9bd45-cfac-485e-a8ee-299026ef6f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "META = Path(\"data/metadata/classification.json\")\n",
    "DEST = Path(\"data/classified\")\n",
    "\n",
    "with open(META, \"r\") as f:\n",
    "    docs = json.load(f)\n",
    "\n",
    "for doc in docs:\n",
    "    if \"file\" not in doc:\n",
    "        continue\n",
    "\n",
    "    src = Path(doc[\"file\"])\n",
    "    category = doc[\"category\"]\n",
    "\n",
    "    dest_dir = DEST / category / src.parent.name\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    shutil.copy2(src, dest_dir / src.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b90377c-fbdc-4379-ac62-cfec0f1941ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Documents saved: 0\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "PDF_ROOT = Path(\"data/classified/text_only\")\n",
    "OUT = Path(\"data/processed\")\n",
    "OUT.mkdir(exist_ok=True)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for pdf_path in PDF_ROOT.rglob(\"*.pdf\"):\n",
    "    print(\"üìÑ\", pdf_path.name)\n",
    "\n",
    "    text_pages = []\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                text_pages.append(text)\n",
    "\n",
    "    full_text = \"\\n\".join(text_pages)\n",
    "\n",
    "    rows.append({\n",
    "        \"dataset\": pdf_path.parts[-2],\n",
    "        \"file\": pdf_path.stem,\n",
    "        \"text\": full_text,\n",
    "        \"char_count\": len(full_text),\n",
    "        \"page_count\": len(text_pages)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUT / \"documents.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Documents saved:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fe11d2-4cc8-435d-849e-c6deba0850b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
